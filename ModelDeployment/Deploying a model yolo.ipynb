{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to deploy a model on Azure ML\n",
    "\n",
    "This notebook takes you through the steps of deploying a model on Azure for The Ocean Cleanup. We can deploy any model registered after the training as seen in the notebooks of the `ModelTraining` directory.\n",
    "\n",
    "Next to the concepts introduced in those notebooks, there are a few additional concepts we need to know about:\n",
    "\n",
    "- score script: A Python script that contains the code of how to perform inference. Implements two functions:\n",
    "  - `init()`: Initializing function run once when the container is started. Initialize your model here\n",
    "  - `run(request)`: Function that is called for every request to the endpoint. This endpoint is called with the\n",
    "    binary image data as body, which is provided to the function through request.get_data(). The response is\n",
    "    sent back.\n",
    "- Service: A deployed model. This provides an `endpoint` to which we can send data.\n",
    "- Endpoint: This is provided by a service. Is an URI to which inference requests can be sent.\n",
    "- InferenceConfig: Configuration describing how the inference should be performed. This contains the score script,\n",
    "  the source it is run in and the environment it should use.\n",
    "- DeployConfig: The configuration of where to deploy the service. This can be a LocalWebservice, AciWebservice or\n",
    "  AksWebservice. The former 2 are aimed at development work, the later for production.\n",
    "- Compute Target: When using an AksWebservice, the Compute Target must be named. This is the AKS cluster where to\n",
    "  run the service. This should be registered as 'Inference cluster' in Azure ML.\n",
    "  \n",
    "Before we get to work with these, we need to load in our workspace again, like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toc_azurewrapper.workspace import get_workspace\n",
    "\n",
    "subscription_id = \"a00eaec6-b320-4e7c-ae61-60a30aec1cfc\"\n",
    "resource_group = \"MachineLearning\"\n",
    "workspace_name = \"RiverImageAnalysis\"\n",
    "tenant_id = \"86f9fea7-9eb0-4325-8b58-7ed0db623956\"\n",
    "\n",
    "workspace = get_workspace(subscription_id, resource_group, workspace_name, tenant_id=tenant_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, like before, we need to create or load an environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-883737b5f1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m environment = get_environment(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"tensorflow-yolo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpip_requirements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./examples/yolo/Tensorflow_YOLO/requirements.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "from toc_azurewrapper.environment import get_environment\n",
    "\n",
    "environment = get_environment(\n",
    "    workspace,\n",
    "    \"tensorflow-yolo\",\n",
    "    pip_requirements=\"./examples/yolo/Tensorflow_YOLO/requirements.txt\",\n",
    "    docker_image=\"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the model. Note that it is possible to provide multiple models to the Service. However, in the current architectural design, we deploy a single model as a single service, and therefore single endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No environment with that name found, creating new one\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model(workspace, \"yolo_v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, the score script should be implemented. The score script implements an `init()` function, and a `run(request)` function. Note that we will deploy the entire folder containing the `score.py` script, so it is perfectly fine to use imports within that and spread your code over multiple files.\n",
    "\n",
    "The provided model is used as follows: The artifacts stored with the model will be loaded into a folder, that is pointed to by the environment variable `AZUREML_MODEL_DIR`. When multiple models are provided, there will be an extra layer of subdirectories there, where each folder has the name of a model used. The artifacts will then be inside that folder.\n",
    "\n",
    "The folder structure is kept the same as shown in the `Artifacts` tab of the Azure ML Studio interface for a model.\n",
    "\n",
    "## Deploying the model\n",
    "\n",
    "Once we have the score script ready, it's time to deploy the model. The function `azurewrapper.deploy.deploy()` can help with this. It will create the correct InferenceConfig and DeployConfig, and will use those to create the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running......................................................................................\n",
      "Succeeded\n",
      "AKS service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from toc_azurewrapper.deploy import deploy\n",
    "service = deploy(\n",
    "    workspace,\n",
    "    \"yolo-v4\",\n",
    "    model,\n",
    "    \"score.py\",\n",
    "    \"examples/yolo\",\n",
    "    environment=environment,\n",
    "    target=\"aks\",\n",
    "    compute_target_name=\"inference-ML\"\n",
    ")\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.scoring_uri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toc_azureml",
   "language": "python",
   "name": "toc_azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
